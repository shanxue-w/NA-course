\documentclass[a4paper]{article}
\usepackage[affil-it]{authblk}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{tikz}
\usepackage{pgfplots}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}


\usepackage{geometry}
\geometry{margin=1.5cm, vmargin={0pt,1cm}}
\setlength{\topmargin}{-1cm}
\setlength{\paperheight}{29.7cm}
\setlength{\textheight}{25.3cm}

\begin{document}

\title{Numerical Analysis homework \# 4}

\author{王昊 Wang Hao 3220104819
  \thanks{Electronic address: \texttt{3220104819@zju.edu.cn}}}
\affil{(Mathematics and Applied Mathematics 2201), Zhejiang University }


\date{Due time: \today}

\maketitle


\section*{I. Convert 477 to FPN}

We have $477 = (111011101)_2 = (1.11011101)_2 \times 2^8$

\section*{II. Convert 0.6 to FPN}

We have $0.6 = (0.100110011001...)_2 \times 2^0 = (1.001100110011...)_2 \times 2^{-1}$

\section*{III. Prove $x_R - x = \beta \qty(x-x_L)$}

Because $x = \beta^e, ~ L<e<U$, we can have $x_L = x - \beta^{e-1} \beta^{1-p}$, $x_R = x + \beta^e \beta^{1-p}$. 

Then we have $x_R - x = \beta^e \beta^{1-p} = \beta \qty(x-x_L)$

\section*{IV. Convert 0.6 to IEEE 754}

For $0.6 = (1.001100110011...)_2 \times 2^{-1}$, we have
\begin{equation}
    \begin{aligned}
        x_L &= (1.00110011001100110011001)_2 \times 2^{-1} \\
        x_R &= (1.00110011001100110011010)_2 \times 2^{-1} \\
        x_R - x &= (0.01100110...)_2 \times 2^{-24} = \frac{2}{5} \times 2^{-24} \\
        x - x_L &= (0.10011001...)_2 \times 2^{-24} = \frac{3}{5} \times 2^{-24} \\
    \end{aligned}
\end{equation}

So $f(x) = 00111111000110011001100110011010$ in IEEE 754, and $\frac{\abs{f(x) - x}}{x} = \frac{2}{3} \times 2^{-24}$

\section*{V. New unit roundoff}

If just simply dropped excess bits, the new unit roundoff would be $\epsilon_u = \epsilon_M = 2^{-23}$, for all numbers in $[1.0, 1.0+\epsilon_M)$ would be rounded to $1.0$, which gives the result. 

\section*{VI. Loss of Precision}

We have $\cos\qty(\frac{1}{4}) = 0.9689124217106447$, so we have
\begin{equation}
   2^{-6} \leq 1 - \frac{\cos\qty(\frac{1}{4})}{1} \leq 2^{-5}
\end{equation}

Therefore, at least 5 bits of precision are lost and at most 6 bits of precision are lost. For $beta=2$ and both inequalities can not be equalities, we have $6$ bits of precision are lost.

\section*{VII. Ways to compute $1-\cos\qty(x)$}

First, we have $1-\cos\qty(x) = 2\sin^2\qty(\frac{x}{2})$, which avoids the subtraction of two nearly equal numbers.

Second, we can use the taylor series, when $x$ is small, we have $1-\cos\qty(x) = \frac{x^2}{2!} - \frac{x^4}{4!} + \frac{x^6}{6!} - \cdots$, and when $x$ is not small, we can directly use $1-\cos\qty(x)$.


\section*{VIII. Condition number of function}

\begin{itemize}
    \item $f(x) = (x-1)^{\alpha}$, we have $\mathrm{cond}_f(x) = \abs{\frac{x f^\prime(x)}{f(x)}} = \abs{\frac{\alpha x}{x-1}}$, when $x$ is close to $1$, the condition number is large.
    \item $f(x) = \ln\qty(x)$, we have $\mathrm{cond}_f(x) = \abs{\frac{x f^\prime(x)}{f(x)}} = \abs{\frac{1}{\ln\qty(x)}}$, when $x$ is close to $1$, the condition number is large.
    \item $f(x) = e^x$, we have $\mathrm{cond}_f(x) = \abs{\frac{x f^\prime(x)}{f(x)}} = \abs{x}$, when $\abs{x}$ is large, the condition number is large.
    \item $f(x) = \arccos\qty(x)$, we have $\mathrm{cond}_f(x) = \abs{\frac{x f^\prime(x)}{f(x)}} = \abs{\frac{x}{\sqrt{1-x^2} \arccos\qty(x)}}$, when $x$ is close to $1$ or $-1$, the condition number is large.
\end{itemize}


\section*{IX. Condition number of particular function}
\subsection*{a. Calculate condition number}
For $f(x) = 1 - e^{-x}$, we have
\begin{equation}
    \text{cond}_f(x) = \abs{\frac{x f^\prime(x)}{f(x)}} = \frac{xe^{-x}}{1-e^{-x}}, \forall x \in [0, 1]. 
\end{equation}

We have $\text{cond}_f^ \prime (x) = \frac{e^{-x}(1-x-e^{-x})}{(1-e^{-x})^2} < 0$, so the condition number is decreasing in $[0, 1]$.

So $\text{cond}_f(x) \leq \text{cond}_f(0) = 1$.

\subsection*{b. Condition number of algorithm}

For the exponential is computed with relative error within machine epsilon, we have
\begin{equation}
    f_A(x) = (1 - (1+\delta_1) e^{-x})(1+\delta_2) = 1 - e^{-x_A} \Rightarrow x_A = -\ln\qty(\qty(1+\delta_1)\qty(1+\delta_2)e^{-x}-\delta_2), \abs{\delta_1}, \abs{\delta_2} \leq \epsilon_u.
\end{equation}

Therefore, we have
\begin{equation}
    \text{cond}_A(x) = \frac{1}{\epsilon_u} \frac{\abs{x_A-x}}{\abs{x}} = \frac{1}{\epsilon_u} \abs{\frac{\ln\qty(\qty(1+\delta_1)\qty(1+\delta_2)-\delta_2 e^x)}{x}} \leq \frac{1}{\epsilon_u} \abs{\frac{\delta_1 + \delta_2 + \delta_1 \delta_2 - \delta_2 e^x}{x}} \leq \frac{e^x}{x}, x\in[0,1].
\end{equation}

\subsection*{c. Plot the condition function}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./plot.png}
\end{figure}

$\text{cond}_A(x)$ tends to infinity as $x$ tends to $0$.


\subsection*{X. Proof of Lemma 4.68}

\begin{equation}
    \begin{aligned}
        \norm{A^{-1}}_2 &= \sup_{x} \frac{\norm{A^{-1}x}_2}{\norm{x}_2} \\
        &= \sup_{y} \frac{\norm{A^{-1}A y}_2}{\norm{Ay}_2} \\
        &= \sup_{y} \frac{\norm{y}_2}{\norm{Ay}_2} \\
        &= \frac{1}{\inf_{\norm{y}_2=1} \norm{Ay}_2} \\
        &= \frac{1}{\sigma_{\min}}.
    \end{aligned}
\end{equation}

Therefore, we have
\begin{equation}
    \text{cond}_2(A) = \norm{A}_2 \norm{A^{-1}}_2 = \frac{\sigma_{\max}}{\sigma_{\min}}.
\end{equation}

\subsection*{XI. Condition number of root finding}

We have
\begin{equation}
    \begin{aligned}
        \frac{\partial f(a_0, \cdots, a_{n-1})}{\partial a_j} &= \lim_{\epsilon \to 0} \frac{f(a_0, \cdots, a_{j-1}, a_j+\epsilon, a_{j+1}, \cdots, a_{n-1}) - f(a_0, \cdots, a_{n-1})}{\epsilon} \\
        &= -\frac{r^j}{p^\prime(r)}~~(\text{root of }p(a_0, \cdots, a_{j-1}, a_j+\epsilon, a_{j+1}, \cdots, a_{n-1}) \text{ is } r-\frac{\epsilon r^j}{p^\prime(r)}).
    \end{aligned}
\end{equation}

So the condition number with 1-norm is 
\begin{equation}
    \mathrm{cond}_f(a) = \sum_{i=0}^{n-1} \abs{a_i \frac{r^i}{p^\prime(r) f(a)}} = \sum_{i=0}^{n-1} \abs{a_i \frac{r^{i-1}}{p^\prime(r)}} = \sum_{i=0}^{n-1} \abs{\frac{a_i r^{i}}{r p^\prime(r)}}
\end{equation}


For Wilkinson example, we have $f(x) = \prod_{i=1}^{p}(x-i)$, for the root $p$, we have 
\begin{equation}
    \sum_{i=0}^{p-1} \abs{a_i p^i} = \sum_{i=0}^{p-1} \abs{a_i} p^i = \prod_{i=1}^{p}(p+i) - p^p.
\end{equation}

Therefore, we have
\begin{equation}
    \mathrm{cond}_f(a) = \frac{\prod_{i=1}^{p}(p+i) - p^p}{p (p-1)!} = \frac{\prod_{i=1}^{p}(p+i) - p^p}{p!}.
\end{equation}

Similiarly, when $p=10, 20, 40$, we have the value of condition number is $1.82 \times 10^5$, $1.38 \times 10^{11}$, $1.08 \times 10^{23}$, which means finding the root of high order polynomial is very difficult, corresponding to the answer of Wilkinson's example.


\section*{XII. Division of two FPNS}

If the division is in a register of precision $2p$. For the binary FPN, we have $\epsilon_M = 2^{1-p}$. Take $p=3, a=(1.00)_2, b=(1.11)_2$, then we have
\begin{equation}
    M_c = \frac{(1.00)_2}{(1.11)_2} = (0.10010)_2, ~~ \beta_c = 0, \Rightarrow c = (1.00)_2 \times 2^{-1} (\text{roundoff}) = 0.5.
\end{equation}

So we have 
\begin{equation}
    \delta = \frac{c}{a/b} - 1 = \frac{1}{8} = \epsilon_u, 
\end{equation}
which contradicts the conclusion that $\abs{\delta} < \epsilon_u$.

\section*{XIII. Bisection accuracy}

First we have $128 = 2^7$, so the step size of FPN is $\delta = \epsilon_M \times 2^7 = 1.52587890625 \times 10^{-5}$, $\frac{1}{2} \delta > 10^{-6}$, so we can't. 

\section*{XIV. Use condition number to explain phenomenon}

For complete cubic spline, we have the following linear system
\begin{equation}
    \begin{pmatrix}
        2 & \mu_2 & & & & \\
        \lambda_3 & 2 & \mu_3 & & & \\
        0 & \lambda_4 & 2 & \mu_4 & &\\
        \vdots & \ddots & \ddots & \ddots & \ddots & \\
        & & & \lambda_{N-2} & 2 & \mu_{N-2} \\
        & & & & \lambda_{N-1} & 2 \\
    \end{pmatrix}
    m = b. 
\end{equation}

In order to explain the phenomenon, we may assume all points are of the same distance, so we have $\lambda_i = \mu_i = \frac{1}{2}$, then we have $\mathrm{cond}_2 A \approx 2.844$, then we assume $x_3 \approx x_4$, then $\mu_3 \approx 1, \lambda_3 \approx 0, \mu_4 \approx 0, \lambda 4 \approx 1$, others doesn't change, then we have $\mathrm{cond}_2 A \approx 3.25$, bigger, so one may get inaccurate results when the distance between two points is much smaller than other adjacent points.

% 非等距节点英文： unequally spaced nodes
For unequally spaced nodes, if the distance between two points is much smaller than other adjacent points, we can get similiar results. 


\section*{Ex4.33}

\subsection*{$b=8.769 \times 10^4$}

(i) $e_c \leftarrow 4$. 
(ii) $m_c = 10.003000$. 
(iii) $m_c = 1.0003000$, $e_c = 5$. 
(iv) do nothing. 
(v) $m_c = 1.000$. 
(vi) $c = 1.000 \times 10^5$. 

\subsection*{$b = -5.678 \times 10^0$}

(i) $m_b = -0.0005678$, $e_c = 4$. 
(ii) $m_c = 1.2334322$. 
(iii) do nothing. 
(iv) do nothing. 
(v) $m_c = 1.233$. 
(vi) $c = 1.233 \times 10^4$. 

\subsection*{$b = -5.678 \times 10^3$}

(i) $m_b = -0.5678$, $e_c = 4$. 
(ii) $m_c = 0.6662000$. 
(iii) $m_c = 6.6620000$, $e_c = 3$. 
(iv) do nothing. 
(v) $m_c = 6.662$. 
(vi) $c = 6.662 \times 10^3$.


\section*{Ex4.42}
Let $a=10^{-10}, c=1$, then we have
\begin{equation}
    \begin{aligned}
        \mathrm{fl} (\sum_{i=0}^{10000} a + c) = 1.00001001358032226562, \\
        \mathrm{fl} (c + \sum_{i=0}^{10000} a) = 1.00000000000000000000.
    \end{aligned}
\end{equation}


\section*{Ex4.43}

We have
\begin{equation}
    \begin{aligned}
        \mathrm{fl} (a_1b_1 + a_2b_2 + a_3b_3) = (1+\delta_4) \qty(a_1b_1 \qty(1+\delta_1) + a_2b_2 \qty(1+\delta_2) + a_3b_3 \qty(1+\delta_3)), \\
        \mathrm{where} \abs{\delta_1}, \abs{\delta_2}, \abs{\delta_3} < \epsilon_u, \abs{\delta_4} < (1+\epsilon_u)^2 \approx 2 \epsilon_u.
    \end{aligned}
\end{equation}

For $\mathrm{fl}\qty( \sum_{i=0}^{m} \prod_{j=0}^{n} a_{i,j} )$, we have
\begin{equation}
    \mathrm{fl}\qty( \sum_{i=0}^{m} \prod_{j=0}^{n} a_{i,j} ) = (1+\delta) \sum_{i=0}^{m} \qty(\prod_{j=0}^{n} a_{i,j} \times \prod_{j=0}^{n-1}(1+\delta_{i,j})),
\end{equation}
where $\abs{\delta} < (1+\epsilon_u)^m \approx m \epsilon_u,~ \abs{\delta_{i,j}} < \epsilon_u$.


\section*{Ex4.80}

It's easy to compute 
\begin{equation}
    \mathrm{cond}_f(x) = \frac{x}{\sin\qty(x)}. 
\end{equation}

We have 
\begin{equation}
    \begin{aligned}
        f_A(x) &= \frac{\sin\qty(x) (1+\delta_1)}{\qty(1+ \cos\qty(x) \qty(1+\delta_2)) \qty(1+\delta_3)} \qty(1+\delta_4) \\
        &= \frac{\sin\qty(x)}{1+\cos\qty(x)}\qty(\qty(1+\delta_1+\delta_4-\delta_3) \qty(1- \frac{\delta_2 \cos\qty(x)}{1+\cos\qty(x)})). ~ (\text{ignore all quadratic terms})
    \end{aligned}
\end{equation}

So we have 
\begin{equation}
    \varphi(x) = 3 + \frac{\cos\qty(x)}{1+\cos\qty(x)}. 
\end{equation}

So by theorem we have 
\begin{equation}
    \mathrm{cond}_A(x) \leq \frac{\sin\qty(x)}{x} \qty(3 + \frac{\cos\qty(x)}{1+\cos\qty(x)}) \leq 2 \pi.
\end{equation}

\end{document}